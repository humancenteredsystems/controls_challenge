# Neural Model Generation and Training Guide

## Overview

This guide provides comprehensive documentation for the neural model generation and training process used in Tournament #3's neural blending system. The system uses 43 specialized ONNX neural models to learn dynamic blending weights for PID controller optimization.

## üß† Neural Model Architecture

### BlenderNet Architecture

The neural blending system uses a simple feedforward network architecture optimized for real-time inference:

```
Input Layer (8 dimensions)
    ‚Üì
Hidden Layer (16 neurons, ReLU activation)
    ‚Üì
Output Layer (1 neuron, Sigmoid activation)
    ‚Üì
Blending Weight (0.0 - 1.0)
```

### Input Features

The neural network processes 8 input features representing the current driving state:

| Feature | Description | Range | Purpose |
|---------|-------------|--------|---------|
| `velocity` | Current vehicle speed | 0-100 mph | Speed-dependent blending |
| `acceleration` | Current vehicle acceleration | -10 to +10 m/s¬≤ | Dynamic response |
| `lateral_error` | Tracking error | -5 to +5 m/s¬≤ | Error magnitude adaptation |
| `error_integral` | Accumulated error | -50 to +50 | Steady-state correction |
| `error_derivative` | Error rate of change | -20 to +20 m/s¬≤/s | Predictive adjustment |
| `future_mean` | Mean future lateral acceleration | -3 to +3 m/s¬≤ | Path planning awareness |
| `future_std` | Future lateral acceleration variance | 0-2 m/s¬≤ | Maneuver complexity |
| `roll_compensation` | Vehicle roll effect | -1 to +1 m/s¬≤ | Dynamic stability |

### Model Specialization

The system generates 43 specialized models, each optimized for specific driving scenarios:

- **Velocity Bins (25 models)**: 0-100 mph in 4 mph increments
- **Maneuver Types (18 models)**: Straight, gentle turns, sharp turns, emergency maneuvers
  - Low-speed turns (0-30 mph)
  - Medium-speed turns (30-60 mph) 
  - High-speed turns (60+ mph)
  - Emergency corrections
  - Straight-line driving
  - Complex multi-turn sequences

## üèóÔ∏è Generation Process

### Script Overview

The neural model generation is handled by [`generate_neural_blending_models.py`](../generate_neural_blending_models.py):

```bash
# Generate all 43 neural models
python generate_neural_blending_models.py

# Generate with custom parameters
python generate_neural_blending_models.py --epochs 200 --batch_size 64 --learning_rate 0.001
```

### Training Data Generation

#### Synthetic Data Generation

The current implementation uses synthetic training data based on velocity patterns:

```python
def generate_training_data(num_samples=10000):
    """Generate synthetic training data for neural blending models"""
    
    # Create diverse driving scenarios
    velocities = np.random.uniform(10, 90, num_samples)
    accelerations = np.random.normal(0, 2, num_samples)
    lateral_errors = np.random.normal(0, 1.5, num_samples)
    
    # Generate features with realistic correlations
    features = np.column_stack([
        velocities,
        accelerations,  
        lateral_errors,
        np.cumsum(lateral_errors * 0.1),  # Error integral
        np.gradient(lateral_errors),       # Error derivative
        np.random.normal(0, 0.8, num_samples),  # Future mean
        np.abs(np.random.normal(0.5, 0.3, num_samples)),  # Future std
        np.random.normal(0, 0.2, num_samples)   # Roll compensation
    ])
    
    # Generate target blending weights based on velocity
    # Low speed (< 40 mph): Favor Tournament #2 parameters (high weight)
    # High speed (> 60 mph): Favor baseline parameters (low weight)
    weights = np.where(velocities < 40, 
                      0.8 + np.random.normal(0, 0.1, num_samples),
                      0.2 + np.random.normal(0, 0.1, num_samples))
    weights = np.clip(weights, 0.0, 1.0)
    
    return features, weights
```

#### Real-World Data Integration (Future Enhancement)

For improved performance, the system should integrate real driving data:

```python
def load_performance_data():
    """Load actual driving performance data from tournament results"""
    
    # Load tournament archive with performance metrics
    with open('plans/tournament_archive.json', 'r') as f:
        archive = json.load(f)
    
    # Extract features and performance for each segment
    training_features = []
    performance_targets = []
    
    for entry in archive['archive']:
        if 'segment_details' in entry:
            for segment in entry['segment_details']:
                features = extract_segment_features(segment)
                performance = calculate_performance_score(segment)
                
                training_features.append(features)
                performance_targets.append(performance)
    
    return np.array(training_features), np.array(performance_targets)
```

### Model Training Pipeline

#### PyTorch Training Implementation

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class BlenderNet(nn.Module):
    """Neural network for learning blending weights"""
    
    def __init__(self, input_dim=8, hidden_dim=16, output_dim=1):
        super(BlenderNet, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Sigmoid()  # Output range [0, 1]
        )
    
    def forward(self, x):
        return self.network(x)

def train_model(features, targets, model_id=0):
    """Train a single neural blending model"""
    
    # Convert to PyTorch tensors
    X = torch.FloatTensor(features)
    y = torch.FloatTensor(targets).reshape(-1, 1)
    
    # Create data loader
    dataset = TensorDataset(X, y)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    # Initialize model
    model = BlenderNet()
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # Training loop
    model.train()
    for epoch in range(100):
        total_loss = 0
        for batch_X, batch_y in dataloader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        if epoch % 20 == 0:
            print(f'Model {model_id}, Epoch {epoch}, Loss: {total_loss:.4f}')
    
    return model
```

#### ONNX Export Process

```python
def export_to_onnx(model, model_path, input_dim=8):
    """Export PyTorch model to ONNX format"""
    
    model.eval()
    
    # Create dummy input for tracing
    dummy_input = torch.randn(1, input_dim)
    
    # Export to ONNX
    torch.onnx.export(
        model,
        dummy_input,
        model_path,
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['features'],
        output_names=['blending_weight'],
        dynamic_axes={
            'features': {0: 'batch_size'},
            'blending_weight': {0: 'batch_size'}
        }
    )
    
    print(f'Model exported to {model_path}')
```

### Validation and Testing

#### Model Validation Pipeline

```python
def validate_model(model_path):
    """Validate ONNX model functionality"""
    
    import onnxruntime as ort
    
    try:
        # Load ONNX model
        session = ort.InferenceSession(model_path)
        
        # Test with sample input
        test_input = np.random.rand(1, 8).astype(np.float32)
        result = session.run(None, {'features': test_input})
        
        # Validate output range
        output = result[0][0][0]
        assert 0.0 <= output <= 1.0, f"Output {output} not in valid range [0,1]"
        
        # Check model size (should be ~1,199 bytes)
        model_size = Path(model_path).stat().st_size
        assert model_size > 1000, f"Model size {model_size} too small (corrupted?)"
        
        return True, f"Model valid: output={output:.3f}, size={model_size} bytes"
        
    except Exception as e:
        return False, f"Validation failed: {e}"
```

## üîß Advanced Training Configurations

### Custom Training Parameters

The generation script supports various training configurations:

```bash
# High-quality training (slower but better results)
python generate_neural_blending_models.py \
    --epochs 500 \
    --batch_size 64 \
    --learning_rate 0.0005 \
    --hidden_dim 32 \
    --validation_split 0.2

# Fast training (development/testing)
python generate_neural_blending_models.py \
    --epochs 50 \
    --batch_size 128 \
    --learning_rate 0.01 \
    --skip_validation

# Performance-optimized training  
python generate_neural_blending_models.py \
    --use_tournament_data \
    --performance_weighted \
    --epochs 300 \
    --early_stopping
```

### Model Architecture Variants

#### Deeper Network Architecture

```python
class DeepBlenderNet(nn.Module):
    """Deeper neural network for complex blending patterns"""
    
    def __init__(self, input_dim=8):
        super(DeepBlenderNet, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(16, 8),
            nn.ReLU(),
            nn.Linear(8, 1),
            nn.Sigmoid()
        )
```

#### Convolutional Feature Extraction

```python
class ConvBlenderNet(nn.Module):
    """Convolutional network for temporal feature extraction"""
    
    def __init__(self, sequence_length=10, input_dim=8):
        super(ConvBlenderNet, self).__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv1d(input_dim, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(16, 8, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1)
        )
        self.fc = nn.Sequential(
            nn.Linear(8, 1),
            nn.Sigmoid()
        )
```

## üìä Training Data Quality Analysis

### Current Limitations

The existing synthetic training data has several limitations affecting performance:

1. **Velocity-Based Patterns**: Models learn simple velocity-to-weight mappings rather than optimal control strategies
2. **Lack of Real Performance Data**: No integration with actual driving performance metrics
3. **Limited Scenario Diversity**: Missing edge cases and complex driving scenarios
4. **No Temporal Relationships**: Features treated independently without sequence modeling

### Performance Impact Analysis

```python
def analyze_training_quality():
    """Analyze the quality and effectiveness of training data"""
    
    # Load current models and test performance
    results = {}
    for i in range(43):
        model_path = f'models/blender_{i}.onnx'
        if Path(model_path).exists():
            # Test model on diverse scenarios
            performance = evaluate_model_scenarios(model_path)
            results[i] = performance
    
    # Statistical analysis
    avg_performance = np.mean(list(results.values()))
    performance_variance = np.var(list(results.values()))
    
    print(f"Average model performance: {avg_performance:.3f}")
    print(f"Performance variance: {performance_variance:.3f}")
    
    # Identify problematic models
    poor_models = [i for i, perf in results.items() if perf < 0.6]
    print(f"Models needing retraining: {poor_models}")
    
    return results
```

## üöÄ Optimization Strategies

### Performance-Based Training

To improve neural model performance, implement performance-based training:

```python
def performance_based_training():
    """Train models using actual performance optimization"""
    
    # Load Tournament #2 champions for high-quality examples
    champions = load_tournament_champions()
    
    # Generate training data from successful parameter combinations
    training_data = []
    for champion in champions:
        # Simulate driving scenarios with champion parameters
        scenarios = generate_driving_scenarios()
        for scenario in scenarios:
            features = extract_features(scenario)
            performance = simulate_performance(scenario, champion['params'])
            
            # Convert performance to blending weight
            # High performance -> use these parameters more (higher weight)
            target_weight = performance_to_weight(performance)
            
            training_data.append((features, target_weight))
    
    return training_data
```

### Reinforcement Learning Integration

For advanced optimization, consider reinforcement learning:

```python
class RLBlenderAgent:
    """Reinforcement learning agent for dynamic blending optimization"""
    
    def __init__(self, state_dim=8, action_dim=1):
        self.actor = BlenderNet(state_dim, 32, action_dim)
        self.critic = nn.Sequential(
            nn.Linear(state_dim + action_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
    
    def select_action(self, state):
        with torch.no_grad():
            return self.actor(state)
    
    def update(self, states, actions, rewards, next_states):
        # Implement DDPG or SAC training
        pass
```

## üîÑ Retraining and Continuous Improvement

### Automated Retraining Pipeline

```python
def setup_retraining_pipeline():
    """Setup automated retraining based on performance feedback"""
    
    # Monitor performance over time
    performance_history = load_performance_history()
    
    # Detect performance degradation
    if detect_performance_decline(performance_history):
        print("Performance decline detected, triggering retraining...")
        
        # Collect new training data from recent runs
        new_data = collect_recent_performance_data()
        
        # Retrain models with updated data
        retrain_models(new_data)
        
        # Validate and deploy updated models
        validate_and_deploy()
```

### Online Learning Integration

```python
def online_learning_update(segment_result):
    """Update models based on real-time performance feedback"""
    
    # Extract features from segment
    features = extract_segment_features(segment_result)
    
    # Calculate actual performance
    actual_performance = calculate_segment_performance(segment_result)
    
    # Convert to training target
    target_weight = performance_to_optimal_weight(actual_performance)
    
    # Update relevant model
    model_id = select_model_for_update(features)
    update_model_online(model_id, features, target_weight)
```

## üìã Best Practices

### Model Development Guidelines

1. **Feature Engineering**
   - Normalize all input features to [-1, 1] range
   - Include temporal derivatives for dynamic response
   - Add domain-specific features (vehicle dynamics, road conditions)

2. **Training Process**
   - Use stratified sampling across velocity ranges
   - Implement early stopping to prevent overfitting
   - Validate on held-out test set before deployment

3. **Model Architecture**
   - Start with simple architectures (8‚Üí16‚Üí1)
   - Add complexity only if justified by performance gains
   - Consider ensemble methods for robustness

4. **Performance Evaluation**
   - Test on diverse driving scenarios
   - Measure both accuracy and inference speed
   - Validate fallback behavior when models fail

### Deployment Considerations

1. **Model Versioning**
   ```bash
   # Tag model versions for rollback capability
   cp -r models/ models_v1.2_backup/
   ```

2. **Gradual Rollout**
   ```python
   # Deploy new models gradually
   def gradual_model_deployment(new_models_dir):
       # Test on subset of scenarios first
       if validate_on_test_scenarios(new_models_dir):
           deploy_to_production(new_models_dir)
       else:
           rollback_to_previous_version()
   ```

3. **Performance Monitoring**
   ```python
   # Monitor model performance in production
   def monitor_model_performance():
       recent_performance = get_recent_performance_metrics()
       if recent_performance < baseline_threshold:
           alert_performance_degradation()
   ```

## üîç Debugging and Troubleshooting

### Common Training Issues

1. **Convergence Problems**
   ```python
   # Debug training convergence
   def debug_training_convergence(loss_history):
       if not is_converging(loss_history):
           print("Training not converging. Try:")
           print("- Reduce learning rate")
           print("- Increase training data")
           print("- Simplify model architecture")
   ```

2. **Overfitting Detection**
   ```python
   # Monitor for overfitting
   def detect_overfitting(train_loss, val_loss):
       if val_loss > train_loss * 1.5:
           print("Overfitting detected:")
           print("- Add dropout layers")
           print("- Reduce model complexity")
           print("- Increase training data")
   ```

3. **Model Export Issues**
   ```python
   # Debug ONNX export
   def debug_onnx_export(model, model_path):
       try:
           export_to_onnx(model, model_path)
           validate_onnx_model(model_path)
       except Exception as e:
           print(f"ONNX export failed: {e}")
           print("Check PyTorch and ONNX versions compatibility")
   ```

## üìà Future Enhancements

### Advanced Model Architectures

1. **Attention Mechanisms**
   - Focus on most relevant input features
   - Dynamic feature weighting based on driving context

2. **Graph Neural Networks**
   - Model relationships between different driving parameters
   - Capture complex interdependencies

3. **Meta-Learning**
   - Quickly adapt to new driving scenarios
   - Few-shot learning for edge cases

### Integration Opportunities

1. **Real-Time Data Collection**
   - Collect performance data during actual runs
   - Use for continuous model improvement

2. **Multi-Objective Optimization**
   - Balance performance, safety, and comfort
   - Pareto-optimal neural blending strategies

3. **Ensemble Methods**
   - Combine multiple neural models
   - Robust predictions with uncertainty quantification

---

**Last Updated:** Tournament #3 Neural Model Generation Documentation  
**Version:** 1.0 - Comprehensive training and generation guide  
**Status:** Production-ready documentation with future enhancement roadmap